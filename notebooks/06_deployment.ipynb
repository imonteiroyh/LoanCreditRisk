{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc39798",
   "metadata": {},
   "source": [
    "# Deployment and Monitoring\n",
    "\n",
    "This notebook provides contextual information on how the credit risk models previously developed (in notebooks 00–05) could be operationalized in a real-world production setting. Rather than hands-on implementation, the intent here is to outline considerations and processes such as preprocessing, inference, shadow testing, monitoring, and retraining triggers.\n",
    "\n",
    "The examples use mocked and derived data to help illustrate key aspects of the pipeline, without accessing production data from LendingClub. The focus remains on process design, workflow, and governance, rather than on specific production details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ff50f",
   "metadata": {},
   "source": [
    "## Problem setup: existing production environment\n",
    "\n",
    "We assume a setting that closely mirrors a real-world credit platform:\n",
    "\n",
    "- There is an existing production model (the champion) that currently drives approval, pricing, and portfolio composition.\n",
    "\n",
    "- Observed outcomes (defaults, repayments) are available only for approved applicants.\n",
    "\n",
    "- Both approval and pricing are policy-driven, creating explicit selection bias in the observed data.\n",
    "\n",
    "- New models (e.g., logistic regression and XGBoost) are evaluated as challengers, but cannot be deployed blindly without careful validation.\n",
    "\n",
    "This setup implies that traditional offline evaluation is insufficient. Any deployment strategy must explicitly account for the fact that we do not observe counterfactual outcomes for applicants who would have been rejected under the incumbent policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7c268",
   "metadata": {},
   "source": [
    "## From offline evaluation to deployment reality\n",
    "\n",
    "In earlier notebooks, we showed that the challenger models provide improved risk ordering within the approved population. However, deployment requires answering a different set of questions:\n",
    "\n",
    "- How do we test a new model without disrupting production decisions?\n",
    "\n",
    "- How do we monitor model behavior when outcomes are only partially observable?\n",
    "\n",
    "- How do we detect drift, degradation, or unintended policy interactions over time?\n",
    "\n",
    "To address these constraints, we adopt a shadow deployment framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179065e1",
   "metadata": {},
   "source": [
    "## Shadow testing under selection bias\n",
    "\n",
    "In shadow mode, the challenger model runs in parallel with the production model, receiving the same inputs and producing predictions, but not influencing decisions.\n",
    "\n",
    "This creates three conceptual regions:\n",
    "\n",
    "- Approved by champion, approved by challenger:\n",
    "\n",
    "    – Fully observable outcomes; safe region for direct comparison.\n",
    "\n",
    "- Approved by champion, rejected by challenger\n",
    "\n",
    "    – Indicates potential risk reduction; outcomes are observable and informative.\n",
    "\n",
    "- Rejected by champion, approved by challenger (shadow region)\n",
    "\n",
    "    – Outcomes are not observed under the current policy, but this region is critical for understanding incremental expansion risk.\n",
    "\n",
    "The third region is particularly important: it represents users the challenger would accept but the production model currently filters out. Because we do not observe outcomes for these applicants, we cannot immediately claim gains or losses. Instead, this region must be monitored indirectly using score distributions, feature drift, stability metrics, and pricing-adjusted risk assumptions, as developed earlier. One practical approach is to randomly approve a small subset of applicants in this shadow region under the new policy. This allows us to collect actual outcome data in this segment without taking on excessive risk or costs, improving our ability to evaluate the true impact of policy changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d59bb",
   "metadata": {},
   "source": [
    "## Moving from Shadow Mode to Production Decisions\n",
    "\n",
    "After a challenger model demonstrates stable and reliable behavior in shadow mode, the key challenge is how to safely transition it into actual credit decision-making. This process requires careful planning to overcome selection bias, outcome delays, and coupled policy effects (approval + pricing).\n",
    "\n",
    "Before allowing the model to directly influence approvals or pricing, several core questions must be addressed:\n",
    "\n",
    "- Does the challenger remain stable under real-world traffic and data drift?\n",
    "- Can we quantify and mitigate risks in segments not observed by the incumbent model (the shadow approval region)?\n",
    "- Have we validated the end-to-end pipeline—including preprocessing, inference, agreement/disagreement analysis, and calibration—under production constraints?\n",
    "- Do we have robust online monitoring to promptly detect unexpected shifts or adverse selection?\n",
    "- Is there a clear rollback plan if portfolio risk rises?\n",
    "\n",
    "The typical path forward is a progressive, controlled rollout. This means:\n",
    "\n",
    "- Routing a small, predefined share of applications (e.g., 5–10%) or specific score bands to the challenger for real decisions.\n",
    "- Observing actual outcomes in these newly accessible regions, while maintaining strict oversight and the ability to revert if necessary.\n",
    "- Using reject inference techniques cautiously to bound potential risks in populations that still lack observed outcomes (e.g., assigning conservative default rates to newly approved applicants).\n",
    "\n",
    "Ultimately, moving a new model into production requires more than offline metrics: it demands live, granular outcome monitoring, rapid feedback loops, and a risk-aware deployment playbook. Only after the challenger proves safety and outperformance under these real production conditions should its deployment share be expanded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29091d",
   "metadata": {},
   "source": [
    "## Experiment Design\n",
    "\n",
    "Offline metrics (AUC, KS, lift) are necessary but not sufficient for production decisions in credit risk. In production, outcomes arrive with delay, approvals are policy-shaped, and profitability depends on pricing and portfolio mix. Experiment design is therefore about answering a causal question as safely as possible:\n",
    "\n",
    "“If we switch decisioning from the champion to the challenger, what changes in outcomes and risk-adjusted performance should we expect — and how confident are we?”\n",
    "\n",
    "Below we outline three practical designs, ordered from strongest causal identification to most governance-friendly for risk.\n",
    "\n",
    "### Classical A/B test (when allowed)\n",
    "\n",
    "A classical A/B test randomizes applicants into two groups:\n",
    "\n",
    "* Group A (Champion): decisions and pricing follow the current production model.\n",
    "* Group B (Challenger): decisions and pricing follow the challenger model.\n",
    "\n",
    "Key properties:\n",
    "\n",
    "* Randomization ensures the two groups are comparable *ex-ante*.\n",
    "* We can estimate causal effects on outcomes (default rate, loss proxies, conversion, etc.).\n",
    "* This is the cleanest way to evaluate the *full policy change* (score + decision threshold + pricing behavior).\n",
    "\n",
    "Practical constraints in credit:\n",
    "\n",
    "* Full A/B may be restricted by compliance or risk appetite.\n",
    "* The riskiest bands may be excluded or downsampled.\n",
    "* Outcome maturity is slow (e.g., 36 months); so we often rely on interim proxies.\n",
    "\n",
    "What we typically measure (early + late):\n",
    "\n",
    "* Early: approval rate, take-up, early delinquency proxies (e.g., 30+ DPD), payment behavior.\n",
    "* Medium: charge-off signals, roll rates, stability metrics.\n",
    "* Late: final default definition, cashflow-based metrics (if available).\n",
    "\n",
    "### Bayesian monitoring (often best for risk)\n",
    "\n",
    "Credit experiments are usually slow and expensive:\n",
    "\n",
    "* Defaults are relatively rare (especially in prime bands),\n",
    "* outcomes take months/years,\n",
    "* and business wants *continuous* signals rather than a single “p-value at the end”.\n",
    "\n",
    "Bayesian monitoring is well-suited because it:\n",
    "\n",
    "* updates evidence continuously as data comes in,\n",
    "* produces probability statements aligned with risk governance (e.g., “80% chance challenger reduces default rate by at least 20 bps”),\n",
    "* supports early stop / early rollback decisions naturally.\n",
    "\n",
    "Typical setup:\n",
    "\n",
    "* Define a prior belief about default rates (based on dev/validation results, shadow mode, historical baseline).\n",
    "* Observe production outcomes as they mature.\n",
    "* Update posterior distributions for champion and challenger.\n",
    "* Track decision-relevant probabilities:\n",
    "\n",
    "  * $P(p_{challenger} < p_{champion})$\n",
    "  * $P(p_{challenger} < p_{champion} - \\delta)$ for a minimum improvement threshold ($\\delta$)\n",
    "  * Expected loss under each, with uncertainty bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91a998",
   "metadata": {},
   "source": [
    "### Mocked Example A — Classical A/B Test (Frequentist)\n",
    "\n",
    "We run a small controlled A/B test for 4–8 weeks in a “safe” population slice (e.g., middle score bands) to avoid extreme-tail exposure. We randomly route applicants to champion vs challenger. Both models decide approvals and apply the associated pricing policy. After enough early outcomes mature (or a proxy like 60+ DPD), we compare default rates.\n",
    "\n",
    "What we do:\n",
    "\n",
    "* Create two groups: A (champion), B (challenger).\n",
    "* Observe ($n_A$, $d_A$) and ($n_B$, $d_B$) where ($d$) is #defaults (or proxy events).\n",
    "* Compute default rates and a statistical significance test for difference in proportions.\n",
    "\n",
    "Decision framing (example):\n",
    "\n",
    "* If challenger reduces default rate by at least X bps and the result is statistically significant, we expand rollout.\n",
    "* If not significant but directionally positive, we extend duration or widen sample.\n",
    "* If significantly worse (or triggers risk thresholds), rollback immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a55e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== A/B Summary ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>n</th>\n",
       "      <th>events</th>\n",
       "      <th>event_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_champion</td>\n",
       "      <td>25000</td>\n",
       "      <td>3256</td>\n",
       "      <td>0.13024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B_challenger</td>\n",
       "      <td>25000</td>\n",
       "      <td>2964</td>\n",
       "      <td>0.11856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          group      n  events  event_rate\n",
       "0    A_champion  25000    3256     0.13024\n",
       "1  B_challenger  25000    2964     0.11856"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Two-proportion z-test (B - A) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delta_B_minus_A</th>\n",
       "      <th>delta_bps</th>\n",
       "      <th>z_value</th>\n",
       "      <th>p_value</th>\n",
       "      <th>ci95_low</th>\n",
       "      <th>ci95_high</th>\n",
       "      <th>decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.01168</td>\n",
       "      <td>-116.8</td>\n",
       "      <td>-3.956715</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.017465</td>\n",
       "      <td>-0.005895</td>\n",
       "      <td>promote_challenger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   delta_B_minus_A  delta_bps   z_value   p_value  ci95_low  ci95_high  \\\n",
       "0         -0.01168     -116.8 -3.956715  0.000076 -0.017465  -0.005895   \n",
       "\n",
       "             decision  \n",
       "0  promote_challenger  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Mocked Example A — Classical A/B Test (Frequentist)\n",
    "# ============================================================\n",
    "\n",
    "# --- Mock experiment setup ---\n",
    "n_total = 50_000\n",
    "traffic_split = 0.5\n",
    "n_A = int(n_total * traffic_split)  # Champion\n",
    "n_B = n_total - n_A  # Challenger\n",
    "\n",
    "# Assume we're measuring an \"early outcome proxy\" (e.g., 60+ DPD) within a short window.\n",
    "# Use plausible rates for an approved population slice.\n",
    "p_A_true = 0.130  # 13.0% event rate under champion\n",
    "p_B_true = 0.122  # 12.2% under challenger (improvement of 80 bps)\n",
    "\n",
    "# Simulate observed events\n",
    "d_A = np.random.binomial(n_A, p_A_true)\n",
    "d_B = np.random.binomial(n_B, p_B_true)\n",
    "\n",
    "pA_hat = d_A / n_A\n",
    "pB_hat = d_B / n_B\n",
    "delta = pB_hat - pA_hat  # challenger - champion (negative is good)\n",
    "\n",
    "# --- Two-proportion z-test (difference in proportions) ---\n",
    "p_pool = (d_A + d_B) / (n_A + n_B)\n",
    "se_pool = np.sqrt(p_pool * (1 - p_pool) * (1 / n_A + 1 / n_B))\n",
    "z = (pB_hat - pA_hat) / se_pool\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "\n",
    "# --- 95% CI for difference (unpooled/Wald) ---\n",
    "se_unpooled = np.sqrt(pA_hat * (1 - pA_hat) / n_A + pB_hat * (1 - pB_hat) / n_B)\n",
    "ci_low = delta - 1.96 * se_unpooled\n",
    "ci_high = delta + 1.96 * se_unpooled\n",
    "\n",
    "# --- Decision logic (example) ---\n",
    "min_improvement = -0.005  # challenger should be at least 50 bps better (delta <= -0.005)\n",
    "alpha = 0.05  # significance threshold\n",
    "\n",
    "decision = \"extend_test\"\n",
    "if (p_value < alpha) and (delta <= min_improvement):\n",
    "    decision = \"promote_challenger\"\n",
    "elif (p_value < alpha) and (delta > 0):\n",
    "    decision = \"rollback_challenger\"\n",
    "elif (p_value >= alpha) and (delta > 0):\n",
    "    decision = \"rollback_challenger\"\n",
    "\n",
    "ab_summary = pd.DataFrame(\n",
    "    [\n",
    "        {\"group\": \"A_champion\", \"n\": n_A, \"events\": d_A, \"event_rate\": pA_hat},\n",
    "        {\"group\": \"B_challenger\", \"n\": n_B, \"events\": d_B, \"event_rate\": pB_hat},\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_stats = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"delta_B_minus_A\": delta,\n",
    "            \"delta_bps\": delta * 10_000,\n",
    "            \"z_value\": z,\n",
    "            \"p_value\": p_value,\n",
    "            \"ci95_low\": ci_low,\n",
    "            \"ci95_high\": ci_high,\n",
    "            \"decision\": decision,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== A/B Summary ===\")\n",
    "display(ab_summary)\n",
    "\n",
    "print(\"=== Two-proportion z-test (B - A) ===\")\n",
    "display(test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0d560",
   "metadata": {},
   "source": [
    "### Mocked Example B — Bayesian Monitoring (Sequential / Risk-Friendly)\n",
    "\n",
    "Before production, we believed the challenger would reduce default rate modestly (e.g., from ~13% to ~12.5%) based on dev metrics and shadow results. We encode that belief as a prior. As production outcomes arrive, we update the posterior continuously and track the probability that challenger is better than champion, with a minimum improvement threshold.\n",
    "\n",
    "What we do:\n",
    "\n",
    "* Use Beta priors for default probabilities:\n",
    "\n",
    "  * $p_A \\sim \\text{Beta}(\\alpha_A, \\beta_A)$\n",
    "  * $p_B \\sim \\text{Beta}(\\alpha_B, \\beta_B)$\n",
    "* Update with observed outcomes:\n",
    "\n",
    "  * Posterior $p \\mid data \\sim \\text{Beta}(\\alpha + d, \\beta + n-d)$\n",
    "* Compute:\n",
    "\n",
    "  * $P(p_B < p_A)$\n",
    "  * $P(p_B < p_A - \\delta)$ for a practical improvement threshold ($\\delta$)\n",
    "\n",
    "Stopping rules (example):\n",
    "\n",
    "* Promote challenger if:\n",
    "\n",
    "  * (P(p_B < p_A - \\delta) > 0.95) (high confidence of meaningful improvement)\n",
    "* Continue monitoring if:\n",
    "\n",
    "  * (0.6 < P(p_B < p_A) < 0.95)\n",
    "* Rollback if:\n",
    "\n",
    "  * (P(p_B > p_A + \\delta_{harm}) > 0.90)\n",
    "\n",
    "This aligns better with credit governance because it communicates risk in probabilistic terms and supports safe incremental decisions without waiting for a single fixed “end of test”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a981d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bayesian Monitoring (weekly updates) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>batch_n_A</th>\n",
       "      <th>batch_events_A</th>\n",
       "      <th>posterior_mean_A</th>\n",
       "      <th>A_ci05</th>\n",
       "      <th>A_ci95</th>\n",
       "      <th>batch_n_B</th>\n",
       "      <th>batch_events_B</th>\n",
       "      <th>posterior_mean_B</th>\n",
       "      <th>B_ci05</th>\n",
       "      <th>B_ci95</th>\n",
       "      <th>P(B &lt; A)</th>\n",
       "      <th>P(B &lt; A - 50bps)</th>\n",
       "      <th>P(B &gt; A + 50bps)</th>\n",
       "      <th>decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2500</td>\n",
       "      <td>305</td>\n",
       "      <td>0.125556</td>\n",
       "      <td>0.117527</td>\n",
       "      <td>0.133773</td>\n",
       "      <td>2500</td>\n",
       "      <td>305</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.115368</td>\n",
       "      <td>0.131489</td>\n",
       "      <td>0.624083</td>\n",
       "      <td>0.345083</td>\n",
       "      <td>0.149708</td>\n",
       "      <td>continue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2500</td>\n",
       "      <td>351</td>\n",
       "      <td>0.130857</td>\n",
       "      <td>0.124288</td>\n",
       "      <td>0.137546</td>\n",
       "      <td>2500</td>\n",
       "      <td>271</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.111721</td>\n",
       "      <td>0.124403</td>\n",
       "      <td>0.989367</td>\n",
       "      <td>0.919058</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>continue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2500</td>\n",
       "      <td>339</td>\n",
       "      <td>0.132105</td>\n",
       "      <td>0.126436</td>\n",
       "      <td>0.137863</td>\n",
       "      <td>2500</td>\n",
       "      <td>292</td>\n",
       "      <td>0.117684</td>\n",
       "      <td>0.112293</td>\n",
       "      <td>0.123167</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>0.975692</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>promote_challenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2500</td>\n",
       "      <td>315</td>\n",
       "      <td>0.130833</td>\n",
       "      <td>0.125805</td>\n",
       "      <td>0.135931</td>\n",
       "      <td>2500</td>\n",
       "      <td>296</td>\n",
       "      <td>0.117833</td>\n",
       "      <td>0.113029</td>\n",
       "      <td>0.122710</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.971075</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>promote_challenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2500</td>\n",
       "      <td>317</td>\n",
       "      <td>0.130138</td>\n",
       "      <td>0.125571</td>\n",
       "      <td>0.134763</td>\n",
       "      <td>2500</td>\n",
       "      <td>282</td>\n",
       "      <td>0.116966</td>\n",
       "      <td>0.112606</td>\n",
       "      <td>0.121385</td>\n",
       "      <td>0.999633</td>\n",
       "      <td>0.982917</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>promote_challenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2500</td>\n",
       "      <td>310</td>\n",
       "      <td>0.129235</td>\n",
       "      <td>0.125028</td>\n",
       "      <td>0.133492</td>\n",
       "      <td>2500</td>\n",
       "      <td>298</td>\n",
       "      <td>0.117294</td>\n",
       "      <td>0.113261</td>\n",
       "      <td>0.121379</td>\n",
       "      <td>0.999608</td>\n",
       "      <td>0.974125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>promote_challenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2500</td>\n",
       "      <td>350</td>\n",
       "      <td>0.130615</td>\n",
       "      <td>0.126668</td>\n",
       "      <td>0.134606</td>\n",
       "      <td>2500</td>\n",
       "      <td>314</td>\n",
       "      <td>0.118359</td>\n",
       "      <td>0.114576</td>\n",
       "      <td>0.122186</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.984733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>promote_challenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2500</td>\n",
       "      <td>369</td>\n",
       "      <td>0.132545</td>\n",
       "      <td>0.128804</td>\n",
       "      <td>0.136325</td>\n",
       "      <td>2500</td>\n",
       "      <td>289</td>\n",
       "      <td>0.118045</td>\n",
       "      <td>0.114487</td>\n",
       "      <td>0.121643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>promote_challenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2500</td>\n",
       "      <td>302</td>\n",
       "      <td>0.131347</td>\n",
       "      <td>0.127815</td>\n",
       "      <td>0.134913</td>\n",
       "      <td>2500</td>\n",
       "      <td>298</td>\n",
       "      <td>0.118163</td>\n",
       "      <td>0.114789</td>\n",
       "      <td>0.121573</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.996825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>promote_challenger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2500</td>\n",
       "      <td>313</td>\n",
       "      <td>0.130778</td>\n",
       "      <td>0.127418</td>\n",
       "      <td>0.134168</td>\n",
       "      <td>2500</td>\n",
       "      <td>317</td>\n",
       "      <td>0.118963</td>\n",
       "      <td>0.115738</td>\n",
       "      <td>0.122220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>promote_challenger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week  batch_n_A  batch_events_A  posterior_mean_A    A_ci05    A_ci95  \\\n",
       "0     1       2500             305          0.125556  0.117527  0.133773   \n",
       "1     2       2500             351          0.130857  0.124288  0.137546   \n",
       "2     3       2500             339          0.132105  0.126436  0.137863   \n",
       "3     4       2500             315          0.130833  0.125805  0.135931   \n",
       "4     5       2500             317          0.130138  0.125571  0.134763   \n",
       "5     6       2500             310          0.129235  0.125028  0.133492   \n",
       "6     7       2500             350          0.130615  0.126668  0.134606   \n",
       "7     8       2500             369          0.132545  0.128804  0.136325   \n",
       "8     9       2500             302          0.131347  0.127815  0.134913   \n",
       "9    10       2500             313          0.130778  0.127418  0.134168   \n",
       "\n",
       "   batch_n_B  batch_events_B  posterior_mean_B    B_ci05    B_ci95  P(B < A)  \\\n",
       "0       2500             305          0.123333  0.115368  0.131489  0.624083   \n",
       "1       2500             271          0.118000  0.111721  0.124403  0.989367   \n",
       "2       2500             292          0.117684  0.112293  0.123167  0.998717   \n",
       "3       2500             296          0.117833  0.113029  0.122710  0.998700   \n",
       "4       2500             282          0.116966  0.112606  0.121385  0.999633   \n",
       "5       2500             298          0.117294  0.113261  0.121379  0.999608   \n",
       "6       2500             314          0.118359  0.114576  0.122186  0.999917   \n",
       "7       2500             289          0.118045  0.114487  0.121643  1.000000   \n",
       "8       2500             298          0.118163  0.114789  0.121573  0.999992   \n",
       "9       2500             317          0.118963  0.115738  0.122220  1.000000   \n",
       "\n",
       "   P(B < A - 50bps)  P(B > A + 50bps)            decision  \n",
       "0          0.345083          0.149708            continue  \n",
       "1          0.919058          0.000783            continue  \n",
       "2          0.975692          0.000017  promote_challenger  \n",
       "3          0.971075          0.000008  promote_challenger  \n",
       "4          0.982917          0.000008  promote_challenger  \n",
       "5          0.974125          0.000000  promote_challenger  \n",
       "6          0.984733          0.000000  promote_challenger  \n",
       "7          0.998883          0.000000  promote_challenger  \n",
       "8          0.996825          0.000000  promote_challenger  \n",
       "9          0.991658          0.000000  promote_challenger  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Mocked Example B — Bayesian Monitoring (Sequential)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "\n",
    "def beta_posterior_params(alpha, beta, n, d):\n",
    "    return alpha + d, beta + (n - d)\n",
    "\n",
    "\n",
    "def beta_mean(alpha, beta):\n",
    "    return alpha / (alpha + beta)\n",
    "\n",
    "\n",
    "def beta_ci(alpha, beta, q=(0.05, 0.95)):\n",
    "    return stats.beta.ppf(q[0], alpha, beta), stats.beta.ppf(q[1], alpha, beta)\n",
    "\n",
    "\n",
    "def prob_B_better(alphaA, betaA, alphaB, betaB, n_mc=200_000, delta=0.0):\n",
    "    # Monte Carlo probability that pB < pA - delta\n",
    "    pA = np.random.beta(alphaA, betaA, size=n_mc)\n",
    "    pB = np.random.beta(alphaB, betaB, size=n_mc)\n",
    "    return np.mean(pB < (pA - delta))\n",
    "\n",
    "\n",
    "# --- Priors informed by dev/shadow beliefs ---\n",
    "# Example belief: champion around 13% and challenger around 12.5%, with moderate confidence.\n",
    "# Using prior strength ~ 2,000 pseudo-observations.\n",
    "prior_strength = 2000\n",
    "pA_prior = 0.130\n",
    "pB_prior = 0.125\n",
    "\n",
    "alphaA0 = pA_prior * prior_strength\n",
    "betaA0 = (1 - pA_prior) * prior_strength\n",
    "alphaB0 = pB_prior * prior_strength\n",
    "betaB0 = (1 - pB_prior) * prior_strength\n",
    "\n",
    "# --- Streaming production results in weekly batches ---\n",
    "weeks = 10\n",
    "batch_n_A = 2500\n",
    "batch_n_B = 2500\n",
    "\n",
    "# True (unknown) underlying event rates in production\n",
    "p_A_prod = 0.131\n",
    "p_B_prod = 0.123\n",
    "\n",
    "records = []\n",
    "alphaA, betaA = alphaA0, betaA0\n",
    "alphaB, betaB = alphaB0, betaB0\n",
    "\n",
    "# Practical thresholds for decisions\n",
    "delta_min = 0.005  # meaningful improvement threshold (50 bps)\n",
    "harm_min = 0.005  # meaningful harm threshold (50 bps)\n",
    "\n",
    "promote_prob = 0.95\n",
    "rollback_prob = 0.90\n",
    "\n",
    "for w in range(1, weeks + 1):\n",
    "    dA = np.random.binomial(batch_n_A, p_A_prod)\n",
    "    dB = np.random.binomial(batch_n_B, p_B_prod)\n",
    "\n",
    "    alphaA, betaA = beta_posterior_params(alphaA, betaA, batch_n_A, dA)\n",
    "    alphaB, betaB = beta_posterior_params(alphaB, betaB, batch_n_B, dB)\n",
    "\n",
    "    meanA = beta_mean(alphaA, betaA)\n",
    "    meanB = beta_mean(alphaB, betaB)\n",
    "    ciA = beta_ci(alphaA, betaA, q=(0.05, 0.95))\n",
    "    ciB = beta_ci(alphaB, betaB, q=(0.05, 0.95))\n",
    "\n",
    "    # Probability challenger is better (any improvement)\n",
    "    p_better = prob_B_better(alphaA, betaA, alphaB, betaB, n_mc=120_000, delta=0.0)\n",
    "\n",
    "    # Probability challenger is better by at least delta_min\n",
    "    p_better_by = prob_B_better(alphaA, betaA, alphaB, betaB, n_mc=120_000, delta=delta_min)\n",
    "\n",
    "    # Probability challenger is worse by at least harm_min\n",
    "    p_worse_by = prob_B_better(\n",
    "        alphaB, betaB, alphaA, betaA, n_mc=120_000, delta=harm_min\n",
    "    )  # P(pA < pB - harm_min)\n",
    "\n",
    "    decision = \"continue\"\n",
    "    if p_better_by > promote_prob:\n",
    "        decision = \"promote_challenger\"\n",
    "    elif p_worse_by > rollback_prob:\n",
    "        decision = \"rollback_challenger\"\n",
    "\n",
    "    records.append(\n",
    "        {\n",
    "            \"week\": w,\n",
    "            \"batch_n_A\": batch_n_A,\n",
    "            \"batch_events_A\": dA,\n",
    "            \"posterior_mean_A\": meanA,\n",
    "            \"A_ci05\": ciA[0],\n",
    "            \"A_ci95\": ciA[1],\n",
    "            \"batch_n_B\": batch_n_B,\n",
    "            \"batch_events_B\": dB,\n",
    "            \"posterior_mean_B\": meanB,\n",
    "            \"B_ci05\": ciB[0],\n",
    "            \"B_ci95\": ciB[1],\n",
    "            \"P(B < A)\": p_better,\n",
    "            \"P(B < A - 50bps)\": p_better_by,\n",
    "            \"P(B > A + 50bps)\": p_worse_by,\n",
    "            \"decision\": decision,\n",
    "        }\n",
    "    )\n",
    "\n",
    "bayes_df = pd.DataFrame(records)\n",
    "\n",
    "print(\"=== Bayesian Monitoring (weekly updates) ===\")\n",
    "display(bayes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d1744",
   "metadata": {},
   "source": [
    "### Shadow Deployment with Targeted Experimentation: A Robust Alternative for High-Risk Rollouts\n",
    "\n",
    "When redirecting a large portion of traffic is costly or operationally challenging, an alternative approach is to combine shadow deployment with selective approval of random cases in the \"shadow\" region. This means that instead of fully diverting users to the new model, you run both the incumbent and challenger models in parallel (the shadow mode), but approve a small random sample of applicants or items in the shadow region for full evaluation. By doing this, you can accurately estimate the conversion rates and risk profiles for each group or policy region, while dramatically limiting the operational risk and resource impact.\n",
    "\n",
    "This targeted experimentation design provides robust statistical power in segments of particular interest (such as the shadow region where models disagree), and allows for clear communication to non-technical stakeholders regarding experimental benefits and risk controls. Especially in environments characterized by high uncertainty or regulatory scrutiny, this approach offers a transparent way to demonstrate the value of further testing, ensures more efficient use of experimental resources, and facilitates more confident, data-driven decision making across all relevant regions of your model's decision space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20985deb",
   "metadata": {},
   "source": [
    "### Practical note on outcome latency\n",
    "\n",
    "Even with A/B or Bayesian monitoring, a 36-month maturity target is slow. In production, we typically:\n",
    "\n",
    "* monitor early delinquency as an interim proxy (30/60/90+ DPD),\n",
    "* validate proxy-to-final mapping using historical vintages,\n",
    "* and use Bayesian monitoring to update confidence as more mature outcomes come in.\n",
    "\n",
    "This avoids premature decisions while still enabling controlled iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10340fa8",
   "metadata": {},
   "source": [
    "## Monitoring Plan\n",
    "\n",
    "Model deployment in credit risk does not end at go-live. Given delayed outcomes, selection bias, and policy coupling, monitoring must focus on leading indicators of degradation, not only on realized defaults. This section outlines a pragmatic monitoring framework that balances robustness, interpretability, and operational feasibility.\n",
    "\n",
    "### Online KPIs\n",
    "\n",
    "Online KPIs track how the model behaves in production traffic, before outcomes fully mature.\n",
    "\n",
    "At a minimum, we monitor:\n",
    "\n",
    "* Approval rate\n",
    "\n",
    "  Sudden changes often indicate upstream data issues, threshold misalignment, or drift in applicant mix.\n",
    "\n",
    "* Score distribution\n",
    "\n",
    "  Mean, variance, and tail mass of predicted risk over time. Sharp shifts may signal feature drift or preprocessing inconsistencies.\n",
    "\n",
    "* Segment-level volume\n",
    "\n",
    "  Distribution of approvals across score bands, grades, or policy buckets. Useful to detect silent policy changes or unintended concentration.\n",
    "\n",
    "* Shadow disagreement rates (if applicable)\n",
    "\n",
    "  Size and composition of:\n",
    "\n",
    "  * champion-approve / challenger-reject\n",
    "  * champion-reject / challenger-approve\n",
    "\n",
    "  Growth in these regions should be explainable and stable.\n",
    "\n",
    "These KPIs are cheap, fast, and should be monitored daily or weekly. In many cases, they catch issues before outcome-based metrics move.\n",
    "\n",
    "### Drift Monitoring\n",
    "\n",
    "Drift refers to changes in the data-generating process that can degrade model performance. We distinguish three main types:\n",
    "\n",
    "#### 1. Data (Covariate) Drift\n",
    "\n",
    "Changes in the distribution of input features:\n",
    "\n",
    "* income, DTI, credit utilization,\n",
    "* inquiry behavior,\n",
    "* geographic or demographic mix.\n",
    "\n",
    "What to monitor:\n",
    "\n",
    "* PSI / CSI for top features,\n",
    "* summary statistics (mean, p95, missingness),\n",
    "* drift by segment (e.g., high vs low score).\n",
    "\n",
    "When basic checks are enough:\n",
    "Small, smooth shifts consistent with seasonality or macro trends.\n",
    "\n",
    "When to go deeper:\n",
    "Large or abrupt shifts, or drift concentrated in high-impact features.\n",
    "\n",
    "#### 2. Prediction (Score) Drift\n",
    "\n",
    "Changes in the distribution of model outputs:\n",
    "\n",
    "* overall risk level,\n",
    "* tail behavior,\n",
    "* separation across bands.\n",
    "\n",
    "Score drift often reflects either input drift or misalignment between preprocessing and inference.\n",
    "\n",
    "What to monitor:\n",
    "\n",
    "* PSI on predicted scores,\n",
    "* share of population in extreme bands,\n",
    "* stability of rank ordering across time.\n",
    "\n",
    "#### 3. Performance Drift (Outcome Drift)\n",
    "\n",
    "Changes in the relationship between predictions and outcomes:\n",
    "\n",
    "* calibration decay,\n",
    "* worsening discrimination,\n",
    "* unexpected segment-level errors.\n",
    "\n",
    "Because outcomes are delayed, we rely on:\n",
    "\n",
    "* early delinquency proxies (e.g., 30/60+ DPD),\n",
    "* vintage curves,\n",
    "* rolling cohort analysis.\n",
    "\n",
    "Key principle:\n",
    "Do not overreact to noise in small windows — use aggregation and Bayesian smoothing when possible.\n",
    "\n",
    "### Early Warning Signals\n",
    "\n",
    "Some signals warrant immediate investigation even before formal thresholds are breached:\n",
    "\n",
    "* Sudden approval rate jumps without a business explanation\n",
    "* Score distribution shifts without corresponding feature drift\n",
    "* Rapid growth of the shadow approval region\n",
    "* Localized calibration gaps in specific segments\n",
    "* Divergence between early delinquency and expected risk\n",
    "\n",
    "These are not necessarily “model failures”, but they often indicate data pipeline issues, policy misalignment, or regime change.\n",
    "\n",
    "### When to Escalate Monitoring\n",
    "\n",
    "Basic monitoring is sufficient when:\n",
    "\n",
    "* drift is gradual,\n",
    "* performance proxies are stable,\n",
    "* changes are explainable by known factors.\n",
    "\n",
    "Deeper investigation is required when:\n",
    "\n",
    "* multiple drift signals align,\n",
    "* changes are asymmetric across segments,\n",
    "* early delinquency diverges persistently from expectations,\n",
    "* or business constraints (loss limits, capital usage) are approached.\n",
    "\n",
    "Escalation may include:\n",
    "\n",
    "* shadow re-evaluation,\n",
    "* temporary tightening of thresholds,\n",
    "* retraining feasibility assessment,\n",
    "* or targeted experimentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
